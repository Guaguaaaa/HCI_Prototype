# backend/llm_service.py
import requests
import json
import re
# å¼•å…¥é…ç½®
from backend.config import OLLAMA_API_URL, MAIN_MODEL_NAME, XAI_MODEL_NAME, SYSTEM_PROMPT, SUMMARY_INTERVAL

# === å…¨å±€å­˜å‚¨ - å‚ä¸è€…ä¼šè¯æ•°æ®éš”ç¦» ===
session_data = {}


def get_session(participant_id: str) -> dict:
    """è·å–æˆ–åˆå§‹åŒ–å‚ä¸è€…çš„ä¼šè¯æ•°æ®"""
    if participant_id not in session_data:
        session_data[participant_id] = {
            'history': [],
            'summary': "",
            'full_prompt': "",
            'turn_count': 0,
            'sentiment_scores': []
        }
    return session_data[participant_id]


def clear_session(participant_id: str) -> bool:
    """æ¸…é™¤ç‰¹å®šå‚ä¸è€…çš„ä¼šè¯å†å²"""
    if participant_id in session_data:
        del session_data[participant_id]
        print(f"ğŸ§¹ Session cleared for PID {participant_id}")
        return True
    return False


def contains_chinese(text: str) -> bool:
    """ç®€å•çš„è¾…åŠ©å‡½æ•°ï¼šæ£€æŸ¥å­—ç¬¦ä¸²æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦"""
    return bool(re.search(r'[\u4e00-\u9fff]', text))


def generate_summary(session: dict):
    """ç”Ÿæˆè¿‘æœŸå¯¹è¯æ‘˜è¦ (ä½¿ç”¨ XAI å°æ¨¡å‹ä»¥èŠ‚çœèµ„æº)"""
    conversation_history = session['history']
    summary_memory = session['summary']

    recent_dialogue = "\n".join(
        [f"{m['role'].capitalize()}: {m['content']}" for m in conversation_history[-10:]]
    )

    summary_prompt = f"""
Please summarize the following conversation into a concise summary of no more than 150 words. 
Focus on the user's main emotions, topics, and intents. Keep the summary in English.

Previous summary (if any):
{summary_memory if summary_memory else "(None)"}

New conversation:
{recent_dialogue}

Output the new summary:
"""
    try:
        resp = requests.post(
            OLLAMA_API_URL,
            json={
                "model": XAI_MODEL_NAME,  # ä½¿ç”¨å°æ¨¡å‹åšæ‘˜è¦
                "prompt": summary_prompt,
                "stream": False
            },
            timeout=120
        )
        resp.raise_for_status()
        data = resp.json()
        new_summary = data.get("response", "").strip()
        if new_summary:
            session['summary'] = new_summary
    except Exception as e:
        print(f"âš ï¸ Failed to generate summary: {e}")


# --- XAI è§£é‡Šç”Ÿæˆå‡½æ•° (åŠ¨æ€ Prompt è¯­è¨€) ---
def generate_xai_explanation(user_text: str, sentiment_data: dict) -> str:
    """
    ä½¿ç”¨å°æ¨¡å‹ç”Ÿæˆ XAI è§£é‡Šã€‚
    æ ¹æ®ç”¨æˆ·è¾“å…¥çš„è¯­è¨€åŠ¨æ€åˆ‡æ¢ Prompt è¯­è¨€ï¼Œç¡®ä¿è¾“å‡ºè¯­è¨€ä¸€è‡´ã€‚
    """
    top_emotion = sentiment_data.get("top_emotion", "neutral")

    # 1. è¯­è¨€æ£€æµ‹ä¸ Prompt åˆ†æµ
    if contains_chinese(user_text):
        # --- ä¸­æ–‡ Prompt ---
        xai_prompt = f"""
        è¯·åˆ†æä»¥ä¸‹ç”¨æˆ·è¾“å…¥å’Œæ£€æµ‹åˆ°çš„æƒ…ç»ªã€‚

        ç”¨æˆ·è¾“å…¥: "{user_text}"
        æ£€æµ‹åˆ°çš„æƒ…ç»ªæ ‡ç­¾: {top_emotion}

        ä»»åŠ¡ï¼š
        1. ç”¨ç¬¬ä¸‰äººç§°ï¼ˆå¦‚â€œç³»ç»Ÿæ£€æµ‹åˆ°...â€ï¼‰ç®€è¦è§£é‡Šä¸ºä»€ä¹ˆç³»ç»Ÿè®¤ä¸ºç”¨æˆ·å¤„äºâ€œ{top_emotion}â€æƒ…ç»ªã€‚
        2. è¯´æ˜ç³»ç»Ÿåœ¨ä¸‹ä¸€æ¡å›å¤ä¸­çš„ç›®æ ‡æ˜¯ä»€ä¹ˆï¼ˆå¦‚â€œç³»ç»Ÿæ—¨åœ¨...â€ï¼‰ã€‚
        3. è§£é‡Šå¿…é¡»ç®€æ´ï¼ˆ1-2å¥è¯ï¼‰ã€‚

        **å¼ºåˆ¶è¦æ±‚**ï¼šå¿…é¡»ä½¿ç”¨**ä¸­æ–‡**ç›´æ¥å›ç­”ï¼Œä¸è¦ç¿»è¯‘ç”¨æˆ·çš„è¯ã€‚
        """
    else:
        # --- English Prompt ---
        xai_prompt = f"""
        Analyze the following user input and the detected emotion.
        
        User Input: "{user_text}"
        Detected Emotion: {top_emotion}
        
        Task: 
        1. Explain briefly (in 1-2 sentences, third person) why the system categorizes the user's emotion as '{top_emotion}'.
        2. State what the goal is for the next response to support them.
        
        **Constraint**: The explanation MUST be in **English**.
        """

    try:
        resp = requests.post(
            OLLAMA_API_URL,
            json={
                "model": XAI_MODEL_NAME,  # ä½¿ç”¨å°æ¨¡å‹ç”Ÿæˆè§£é‡Š
                "prompt": xai_prompt,
                "stream": False,
                "options": {
                    "temperature": 0.3,
                    "max_tokens": 150
                }
            },
            timeout=10
        )
        if resp.status_code == 200:
            return resp.json().get("response", "").strip()
        return "System analysis unavailable."
    except Exception as e:
        print(f"âš ï¸ XAI Gen Error: {e}")
        return "System analysis unavailable."


# --- MODIFIED: ä¸»å¯¹è¯ç”Ÿæˆå‡½æ•° (åŠ¨æ€ System Prompt) ---
def get_llm_response_stream(participant_id: str, user_input: str):
    """
    å¤„ç†èŠå¤©é€»è¾‘å’Œ LLM å“åº”æµ (ä½¿ç”¨ä¸»æ¨¡å‹)ã€‚
    """
    session = get_session(participant_id)
    conversation_history = session['history']
    summary_memory = session['summary']

    # 1. æ·»åŠ ç”¨æˆ·è¾“å…¥
    conversation_history.append({"role": "user", "content": user_input})

    # 2. åŠ¨æ€å†³å®š System Prompt (è¯­è¨€è·Ÿéš)
    # å¦‚æœæ£€æµ‹åˆ°ä¸­æ–‡è¾“å…¥ï¼Œå¼ºåˆ¶ä½¿ç”¨ä¸­æ–‡ System Prompt
    if contains_chinese(user_input):
        current_system_prompt = (
            "ä½ æ˜¯ä¸€ä¸ªæ¸©æŸ”ä¸”å¯Œæœ‰åŒç†å¿ƒçš„å¯¹è¯ä¼™ä¼´ã€‚"
            "è¯·å§‹ç»ˆä»¥è‡ªç„¶ã€åƒäººä¸€æ ·çš„æ–¹å¼å›åº”ã€‚"
            "è¯·åŠ¡å¿…ä½¿ç”¨ä¸­æ–‡è¿›è¡Œå›å¤ã€‚"
            "ä¸è¦è¯„ä»·ç”¨æˆ·çš„è¯­è¨€èƒ½åŠ›ã€‚"
        )
    else:
        # è‹±æ–‡è¾“å…¥åˆ™ä½¿ç”¨é»˜è®¤é…ç½® (è‹±æ–‡)
        current_system_prompt = SYSTEM_PROMPT

    # 3. æ„å»º Prompt
    full_prompt = ""

    # --- FIX: å§‹ç»ˆåœ¨ Prompt å¼€å¤´åŒ…å« System Prompt ---
    # ä¹‹å‰çš„é€»è¾‘æ˜¯åªåœ¨ len==1 æ—¶æ·»åŠ ï¼Œå¯¼è‡´åç»­è½®æ¬¡ System Prompt ä¸¢å¤±
    full_prompt += current_system_prompt + "\n\n"

    if summary_memory:
        full_prompt += f"Context Summary:\n{summary_memory}\n\n"

    for msg in conversation_history[-10:]:
        prefix = "User:" if msg["role"] == "user" else "AI:"
        full_prompt += f"{prefix} {msg['content']}\n"

    full_prompt += "AI:"

    # å­˜å…¥ Session ä»…ä¾›è°ƒè¯•æŸ¥çœ‹
    session['full_prompt'] = full_prompt

    # --- æµå¼å“åº” (ä½¿ç”¨ MAIN_MODEL_NAME) ---
    full_ai_reply = ""
    try:
        response = requests.post(
            OLLAMA_API_URL,
            json={
                "model": MAIN_MODEL_NAME,
                "prompt": full_prompt,
                "stream": True
            },
            stream=True,
            timeout=300
        )
        response.raise_for_status()

        for line in response.iter_lines():
            if line:
                try:
                    json_line = line.decode('utf-8')
                    data = json.loads(json_line)
                    text_chunk = data.get("response", "")
                    if text_chunk:
                        full_ai_reply += text_chunk
                        yield text_chunk.encode('utf-8')
                    if data.get("done", False):
                        break
                except json.JSONDecodeError:
                    pass

    except requests.RequestException as e:
        yield f"âš ï¸ Backend LLM error: {e}".encode('utf-8')

    finally:
        if full_ai_reply:
            conversation_history.append({"role": "ai", "content": full_ai_reply.strip()})
            session['turn_count'] += 1
            if len(conversation_history) % (SUMMARY_INTERVAL * 2) == 0:
                generate_summary(session)
        print("âœ… Streaming Complete")