# backend/llm_service.py
import requests
import json
# å¼•å…¥æ–°çš„é…ç½®å˜é‡å
from backend.config import OLLAMA_API_URL, MAIN_MODEL_NAME, XAI_MODEL_NAME, SYSTEM_PROMPT, SUMMARY_INTERVAL

# === å…¨å±€å­˜å‚¨ - å‚ä¸è€…ä¼šè¯æ•°æ®éš”ç¦» ===
session_data = {}


def get_session(participant_id: str) -> dict:
    """è·å–æˆ–åˆå§‹åŒ–å‚ä¸è€…çš„ä¼šè¯æ•°æ®"""
    if participant_id not in session_data:
        session_data[participant_id] = {
            'history': [],
            'summary': "",
            'full_prompt': "",
            'turn_count': 0,
            'sentiment_scores': []
        }
    return session_data[participant_id]


def clear_session(participant_id: str) -> bool:
    """æ¸…é™¤ç‰¹å®šå‚ä¸è€…çš„ä¼šè¯å†å²"""
    if participant_id in session_data:
        del session_data[participant_id]
        print(f"ğŸ§¹ Session cleared for PID {participant_id}")
        return True
    return False


def generate_summary(session: dict):
    """ç”Ÿæˆè¿‘æœŸå¯¹è¯æ‘˜è¦ (ä½¿ç”¨ XAI å°æ¨¡å‹ä»¥èŠ‚çœèµ„æº)"""
    conversation_history = session['history']
    summary_memory = session['summary']

    recent_dialogue = "\n".join(
        [f"{m['role'].capitalize()}: {m['content']}" for m in conversation_history[-10:]]
    )

    summary_prompt = f"""
Please summarize the following conversation into a concise summary of no more than 150 words. 
Focus on the user's main emotions, topics, and intents. Keep the summary in English.

Previous summary (if any):
{summary_memory if summary_memory else "(None)"}

New conversation:
{recent_dialogue}

Output the new summary:
"""
    try:
        resp = requests.post(
            OLLAMA_API_URL,
            json={
                "model": XAI_MODEL_NAME,  # ä½¿ç”¨å°æ¨¡å‹åšæ‘˜è¦
                "prompt": summary_prompt,
                "stream": False
            },
            timeout=120
        )
        resp.raise_for_status()
        data = resp.json()
        new_summary = data.get("response", "").strip()
        if new_summary:
            session['summary'] = new_summary
    except Exception as e:
        print(f"âš ï¸ Failed to generate summary: {e}")


# --- NEW: XAI è§£é‡Šç”Ÿæˆå‡½æ•° ---
def generate_xai_explanation(user_text: str, sentiment_data: dict) -> str:
    """
    ä½¿ç”¨å°æ¨¡å‹ç”Ÿæˆ XAI è§£é‡Šã€‚
    è§£é‡ŠåŒ…å«ï¼šå¯¹ç”¨æˆ·æƒ…ç»ªçš„è¯†åˆ« + AI æ„å›¾çš„ç®€è¿°ã€‚
    """
    top_emotion = sentiment_data.get("top_emotion", "neutral")

    # æ„é€  XAI Prompt
    # è¿™æ˜¯ä¸€ä¸ª Meta-Promptï¼Œè®© AI è§£é‡Šè‡ªå·±çš„â€œå†…éƒ¨çŠ¶æ€â€
    xai_prompt = f"""
Analyze the following user input and the detected emotion.
User Input: "{user_text}"
Detected Emotion: {top_emotion}

Task: Explain briefly (in 1-2 sentences) why you categorize the user's emotion as '{top_emotion}' and what your goal is for the next response to support them. 
Write the explanation in the third person (e.g., "The system detects...", "The agent aims to...").
Keep it concise and objective.
"""

    try:
        resp = requests.post(
            OLLAMA_API_URL,
            json={
                "model": XAI_MODEL_NAME,  # ä½¿ç”¨å°æ¨¡å‹ç”Ÿæˆè§£é‡Š
                "prompt": xai_prompt,
                "stream": False,
                "options": {
                    "temperature": 0.3,  # ä¿æŒè§£é‡Šçš„ç¨³å®šæ€§
                    "max_tokens": 100
                }
            },
            timeout=10
        )
        if resp.status_code == 200:
            return resp.json().get("response", "").strip()
        return "System analysis unavailable."
    except Exception as e:
        print(f"âš ï¸ XAI Gen Error: {e}")
        return "System analysis unavailable."


def get_llm_response_stream(participant_id: str, user_input: str):
    """
    å¤„ç†èŠå¤©é€»è¾‘å’Œ LLM å“åº”æµ (ä½¿ç”¨ä¸»æ¨¡å‹)ã€‚
    """
    session = get_session(participant_id)
    conversation_history = session['history']
    summary_memory = session['summary']

    # 1. æ·»åŠ ç”¨æˆ·è¾“å…¥
    conversation_history.append({"role": "user", "content": user_input})

    # --- æ„å»º Prompt ---
    full_prompt = ""
    if len(conversation_history) == 1:
        full_prompt += SYSTEM_PROMPT + "\n\n"

    if summary_memory:
        full_prompt += f"Context Summary:\n{summary_memory}\n\n"

    for msg in conversation_history[-10:]:
        prefix = "User:" if msg["role"] == "user" else "AI:"
        full_prompt += f"{prefix} {msg['content']}\n"

    full_prompt += "AI:"
    session['full_prompt'] = full_prompt

    # --- æµå¼å“åº” (ä½¿ç”¨ MAIN_MODEL_NAME) ---
    full_ai_reply = ""
    try:
        response = requests.post(
            OLLAMA_API_URL,
            json={
                "model": MAIN_MODEL_NAME,  # ä½¿ç”¨å¤§æ¨¡å‹è¿›è¡Œå¯¹è¯
                "prompt": full_prompt,
                "stream": True
            },
            stream=True,
            timeout=300
        )
        response.raise_for_status()

        for line in response.iter_lines():
            if line:
                try:
                    json_line = line.decode('utf-8')
                    data = json.loads(json_line)
                    text_chunk = data.get("response", "")
                    if text_chunk:
                        full_ai_reply += text_chunk
                        yield text_chunk.encode('utf-8')
                    if data.get("done", False):
                        break
                except json.JSONDecodeError:
                    pass

    except requests.RequestException as e:
        yield f"âš ï¸ Backend LLM error: {e}".encode('utf-8')

    finally:
        if full_ai_reply:
            conversation_history.append({"role": "ai", "content": full_ai_reply.strip()})
            session['turn_count'] += 1
            if len(conversation_history) % (SUMMARY_INTERVAL * 2) == 0:
                generate_summary(session)
        print("âœ… Streaming Complete")